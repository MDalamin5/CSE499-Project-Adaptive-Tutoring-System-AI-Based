{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "67a0998d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableLambda, RunnableBranch, RunnableSequence\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from langchain_groq import ChatGroq\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "\n",
    "groq_api_key=os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "model=ChatGroq(groq_api_key=groq_api_key,model_name=\"deepseek-r1-distill-llama-70b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "04dbbb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StuOutputCorrectness(BaseModel):\n",
    "    sentiment: Literal['correct', 'partially_correct', 'incorrect'] = Field(description='Give the sentiment from the student response')\n",
    "    \n",
    "parser = PydanticOutputParser(pydantic_object=StuOutputCorrectness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7cb91c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_messages = \"You are a helpful assistant\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "db55dead",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_messages),\n",
    "        (\"human\", \n",
    "         \"Analyze the following student response and classify its correctness. \"\n",
    "         \"Determine whether it is 'correct', 'partially_correct', or 'incorrect'.\\n\\n\"\n",
    "         \"Student Response: {response}\\n\\n\"\n",
    "         \"Provide the classification following this format:\\n{format_instruction}\")\n",
    "    ]\n",
    ").partial(format_instruction=parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d8af7fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_chain = chat_prompt | model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f6fc241e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StuOutputCorrectness(sentiment='incorrect')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_chain.invoke(\n",
    "    {\n",
    "        'response': \"This sum of 2+2=5\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4a5fb9",
   "metadata": {},
   "source": [
    "## ***Adaptive Prompt With Messages History***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "51ed880b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"Analyze the following student response and classify its correctness.\"\n",
    "         \"Determine whether it is 'correct', 'partially_correct', or 'incorrect'.\\n\\n\"\n",
    "         \"Student Response: {response}\\n\\n\"\n",
    "         \"Provide the classification following this format:\\n{format_instruction}\"),\n",
    "        MessagesPlaceholder(variable_name='messages')\n",
    "    ]\n",
    ").partial(format_instruction=parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c55e66d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_chain = chat_prompt | model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2e662c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## List of messages\n",
    "messages = [\n",
    "                HumanMessage(content=\"solve this problem and 2+2=0?\"),\n",
    "                AIMessage(content=\"This is a addition problem and think about you have tow apple and your friend also give 2 more apple now how many apple you have. Its a hints to solve this problem.\"),\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "10d23f94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StuOutputCorrectness(sentiment='partially_correct')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_chain.invoke(\n",
    "    {\n",
    "        \n",
    "        'messages': messages,\n",
    "        'response':\"maybe i have 3 apple or 4 i'm bit confused. so 2+2=4\",\n",
    "        \n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2387dbb7",
   "metadata": {},
   "source": [
    "## Add Branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "01543b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt_correct = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \n",
    "         \"Student gave the correct response and the response is {response}. \"\n",
    "         \"Now give an adaptive prompt so the model can generate only the best response for the next step. \"\n",
    "         \"Also analyze previous message history — it will help generate a better adaptive prompt.\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt_partially_correct = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \n",
    "         \"\"\"\n",
    "         your task is generate a best adaptive prompt. And this adaptive prompt goes to my main model as the system prompt. Using this prompt the main model will be generated the elaborated response you just generate one the adaptive prompt only. Think you just help to main model as a instructor like a boss.\n",
    "         for generate one best adaptive prompt you can analysis the previous chat and current response also.\n",
    "         Student gave a response which is partially correct and the response is {response}. \n",
    "         \"\"\"\n",
    "         ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt_incorrect = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \n",
    "         \"Student gave a totally incorrect response and the response is {response}. \"\n",
    "         \"Analyze the student’s misunderstanding — maybe they need clarification on core concepts. \"\n",
    "         \"Provide elaborated hints or ask guiding questions. \"\n",
    "         \"Use previous message history to craft the best adaptive prompt for the next step.\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2dab5387",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "688c651d",
   "metadata": {},
   "outputs": [],
   "source": [
    "branch_chain = RunnableBranch(\n",
    "    (lambda x: x[\"sentiment\"] == 'correct', prompt_correct | model | str_parser),\n",
    "    (lambda x: x[\"sentiment\"] == 'partially_correct', prompt_partially_correct | model | str_parser),\n",
    "    (lambda x: x[\"sentiment\"] == 'incorrect', prompt_incorrect | model | str_parser),\n",
    "    RunnableLambda(lambda x: \"Couldn't determine sentiment.\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f98b2fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# Wrap classifier_chain to return full input + parsed sentiment\n",
    "def enrich_with_sentiment(input: dict) -> dict:\n",
    "    sentiment_result = classifier_chain.invoke(input)\n",
    "    print(sentiment_result)\n",
    "    return {\n",
    "        **input,  # includes 'messages' and 'response'\n",
    "        'sentiment': sentiment_result.sentiment\n",
    "    }\n",
    "\n",
    "enriched_chain = RunnableLambda(enrich_with_sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d15e8167",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_chain = enriched_chain | branch_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7918a4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment='correct'\n",
      " Now solve the problem 2+2=?\n",
      "</think><think>\n",
      "Okay, so the user is trying to solve the problem 2+2=?. They mentioned something about apples, like if I have two apples and my friend gives me two more, how many do I have? That makes sense because adding apples is a common way to explain addition. \n",
      "\n",
      "But wait, in their initial message, they said maybe they have 3 apples or 4 and they're confused. So I guess they're not sure if it's 3 or 4. Then they said 2+2=4. Hmm, but then they asked why is 2+2=0? That's confusing because 2+2 is definitely 4, not 0.\n",
      "\n",
      "I think the user might be mixing up different math concepts. Maybe they heard somewhere that 2+2 can be something else, like in binary or another context, but in basic arithmetic, it's always 4. Or maybe they're just trying to trick me, but I don't think so. They seem genuinely confused.\n",
      "\n",
      "I should probably explain that in regular addition, 2+2 equals 4. Using the apple example helps visualize it: starting with two apples and getting two more makes four apples total. It's a straightforward addition problem, so there's no reason to think it equals zero unless it's a different operation or context, which they didn't mention.\n",
      "\n",
      "I should make sure to clarify this in a friendly way, maybe reassure them that it's okay to be confused sometimes, but in this case, 2+2 is definitely 4. I can also encourage them to ask more questions if they're still unsure.\n",
      "</think>\n",
      "\n",
      "The correct answer to the problem \\(2 + 2 = ?\\) is 4. \n",
      "\n",
      "**Step-by-Step Explanation:**\n",
      "\n",
      "1. **Understanding the Problem:** You start with 2 apples.\n",
      "2. **Adding More Apples:** Your friend gives you an additional 2 apples.\n",
      "3. **Calculating the Total:** To find the total number of apples, you add the initial 2 apples to the 2 apples received: \\(2 + 2 = 4\\).\n",
      "\n",
      "Thus, the total number of apples you have is **4**. If you have any more questions or need further clarification, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "adaptive_prompt = final_chain.invoke(\n",
    "    {\n",
    "        \n",
    "        'messages': messages,\n",
    "        'response':\"maybe i have 3 apple or 4 i'm bit confused. so 2+2=4\",\n",
    "        \n",
    "    }\n",
    ")\n",
    "print(adaptive_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "eec272a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment='correct'\n",
      " So 2+2=4. So 2+2=4 is correct. But if you think 2+2=0 this is wrong. So the correct answer is 4.\n",
      "\n",
      "Okay, so the user is asking about 2+2=0 and seems confused. They mentioned having apples but are unsure if it's 3 or 4. Earlier, they thought the answer was 4, but now they're considering 0. Hmm, maybe they're overcomplicating it. I should guide them back to the basics.\n",
      "\n",
      "I need to create an adaptive prompt that helps the model address their confusion. Let me break it down: acknowledge their uncertainty, use the apple analogy again, explain why 4 is correct, and correct the misunderstanding about 0. Keep it clear and reassuring so they feel confident in the answer.\n",
      "\n",
      "Alright, let me structure the prompt to cover these points. Make sure it's concise and leads the model to provide a supportive and educational response.\n",
      "</think>\n",
      "\n",
      "Certainly! Here's an adaptive prompt based on the previous message history:\n",
      "\n",
      "---\n",
      "\n",
      "**Adaptive Prompt:**  \n",
      "\"Recognize that the user is confused about the result of 2+2, having previously thought it might be 3 or 4. They also mentioned 2+2=0, which is incorrect. Use the apple analogy to clarify: if you have 2 apples and receive 2 more, you have 4 apples in total. Explain that 2+2=4 is the correct answer and gently correct the misunderstanding about 2+2=0. Provide a reassuring and clear explanation to help the user feel confident in the answer.\"\n",
      "\n",
      "---\n",
      "\n",
      "This prompt ensures the model addresses the user's confusion, uses the apple analogy for clarity, and provides a supportive response.\n"
     ]
    }
   ],
   "source": [
    "adaptive_prompt2 = final_chain.invoke(\n",
    "    {\n",
    "        \n",
    "        'messages': messages,\n",
    "        'response':\"maybe i have 3 apple or 4 i'm bit confused. so 2+2=4\",\n",
    "        \n",
    "    }\n",
    ")\n",
    "print(adaptive_prompt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49e607b",
   "metadata": {},
   "source": [
    "## get structure adaptive prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8ce06814",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptivePrompt(BaseModel):\n",
    "    adaptive_prompt: str = Field(description=\"The final adaptive prompt to guide the main model.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "258b0c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptive_parser = PydanticOutputParser(pydantic_object=AdaptivePrompt)\n",
    "\n",
    "prompt_partially_correct = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \n",
    "         \"\"\"\n",
    "         Your task is to generate one adaptive prompt only. This prompt will be used as the system prompt for the main model. \n",
    "         Think of yourself as a supervisor providing the best instruction for the AI tutor.\n",
    "\n",
    "         You can analyze the previous chat history and the student’s latest response.\n",
    "\n",
    "         Student gave a response which is partially correct: {response}\n",
    "\n",
    "         Return your result in the following format:\n",
    "         {format_instructions}\n",
    "         \"\"\"\n",
    "         ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ").partial(format_instructions=adaptive_parser.get_format_instructions())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "74d67457",
   "metadata": {},
   "outputs": [],
   "source": [
    "branch_chain = RunnableBranch(\n",
    "    (lambda x: x[\"sentiment\"] == 'correct', prompt_correct | model | adaptive_parser),\n",
    "    (lambda x: x[\"sentiment\"] == 'partially_correct', prompt_partially_correct | model | adaptive_parser),\n",
    "    (lambda x: x[\"sentiment\"] == 'incorrect', prompt_partially_correct | model | adaptive_parser),\n",
    "    RunnableLambda(lambda x: {\"adaptive_prompt\": \"Couldn't determine sentiment.\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "07d51b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# Wrap classifier_chain to return full input + parsed sentiment\n",
    "def enrich_with_sentiment(input: dict) -> dict:\n",
    "    sentiment_result = classifier_chain.invoke(input)\n",
    "    print(sentiment_result)\n",
    "    return {\n",
    "        **input,  # includes 'messages' and 'response'\n",
    "        'sentiment': sentiment_result.sentiment\n",
    "    }\n",
    "\n",
    "enriched_chain = RunnableLambda(enrich_with_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0ded50c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_chain = enriched_chain | branch_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "58f48fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment='incorrect'\n",
      "adaptive_prompt=\"Let's solve the problem step by step. If you have 2 apples and your friend gives you 2 more, you now have a total of 4 apples. So, 2 + 2 equals 4, not 0. Addition is about combining groups, so when you add 2 and 2, you get 4.\"\n"
     ]
    }
   ],
   "source": [
    "adaptive_prompt3 = final_chain.invoke(\n",
    "    {\n",
    "        \n",
    "        'messages': messages,\n",
    "        'response':\"maybe i have 3 apple or 4 i'm bit confused. so 2+2=4\",\n",
    "        \n",
    "    }\n",
    ")\n",
    "print(adaptive_prompt3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "39ad4074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+  \n",
      "| enrich_with_sentiment_input |  \n",
      "+-----------------------------+  \n",
      "                *                \n",
      "                *                \n",
      "                *                \n",
      "    +--------------------+       \n",
      "    | ChatPromptTemplate |       \n",
      "    +--------------------+       \n",
      "                *                \n",
      "                *                \n",
      "                *                \n",
      "          +----------+           \n",
      "          | ChatGroq |           \n",
      "          +----------+           \n",
      "                *                \n",
      "                *                \n",
      "                *                \n",
      "    +----------------------+     \n",
      "    | PydanticOutputParser |     \n",
      "    +----------------------+     \n",
      "                *                \n",
      "                *                \n",
      "                *                \n",
      "          +--------+             \n",
      "          | Branch |             \n",
      "          +--------+             \n",
      "                *                \n",
      "                *                \n",
      "                *                \n",
      "        +--------------+         \n",
      "        | BranchOutput |         \n",
      "        +--------------+         \n"
     ]
    }
   ],
   "source": [
    "final_chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6114d76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchGPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
